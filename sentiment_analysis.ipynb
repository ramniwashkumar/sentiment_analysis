{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "662d772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183526</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>Such a great idea! very handy to have and look...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183527</th>\n",
       "      <td>Baby Teething Necklace for Mom Pretty Donut Sh...</td>\n",
       "      <td>This product rocks!  It is a great blend of fu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183528</th>\n",
       "      <td>Abstract 2 PK Baby / Toddler Training Cup (Pink)</td>\n",
       "      <td>This item looks great and cool for my kids.......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183529</th>\n",
       "      <td>Baby Food Freezer Tray - Bacteria Resistant, B...</td>\n",
       "      <td>I am extremely happy with this product. I have...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183530</th>\n",
       "      <td>Best 2 Pack Baby Car Shade for Kids - Window S...</td>\n",
       "      <td>I love this product very mush . I have bought ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183531 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "0                                Planetwise Flannel Wipes   \n",
       "1                                   Planetwise Wipe Pouch   \n",
       "2                     Annas Dream Full Quilt with 2 Shams   \n",
       "3       Stop Pacifier Sucking without tears with Thumb...   \n",
       "4       Stop Pacifier Sucking without tears with Thumb...   \n",
       "...                                                   ...   \n",
       "183526  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183527  Baby Teething Necklace for Mom Pretty Donut Sh...   \n",
       "183528   Abstract 2 PK Baby / Toddler Training Cup (Pink)   \n",
       "183529  Baby Food Freezer Tray - Bacteria Resistant, B...   \n",
       "183530  Best 2 Pack Baby Car Shade for Kids - Window S...   \n",
       "\n",
       "                                                   review  rating  \n",
       "0       These flannel wipes are OK, but in my opinion ...       3  \n",
       "1       it came early and was not disappointed. i love...       5  \n",
       "2       Very soft and comfortable and warmer than it l...       5  \n",
       "3       This is a product well worth the purchase.  I ...       5  \n",
       "4       All of my kids have cried non-stop when I trie...       5  \n",
       "...                                                   ...     ...  \n",
       "183526  Such a great idea! very handy to have and look...       5  \n",
       "183527  This product rocks!  It is a great blend of fu...       5  \n",
       "183528  This item looks great and cool for my kids.......       5  \n",
       "183529  I am extremely happy with this product. I have...       5  \n",
       "183530  I love this product very mush . I have bought ...       5  \n",
       "\n",
       "[183531 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'D:\\amazon_baby.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93102fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c7988b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165191</th>\n",
       "      <td>aden + anais Rayon From Bamboo Crib Sheet, Azu...</td>\n",
       "      <td>An off-white or cream sheet that is so soft. I...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108775</th>\n",
       "      <td>Hand Held Scalp Head Massager - Set of Three (...</td>\n",
       "      <td>I was skeptical about how well these will work...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162820</th>\n",
       "      <td>Clevamama ClevaFoam Baby Pillow, Cream</td>\n",
       "      <td>It soft and material appears to be excellent. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148217</th>\n",
       "      <td>Kids Line Velour Changing Pad Cover, Pirate Party</td>\n",
       "      <td>This is a very nice cover. I have two because ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46428</th>\n",
       "      <td>Angel Dear Blankie, Green Frog</td>\n",
       "      <td>I love these Lovies. They are cute, soft and d...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137608</th>\n",
       "      <td>Trend Lab CribWrap Fleece Rail Cover for Long ...</td>\n",
       "      <td>Fit my Jenny Lind crib perfectly. Water proof ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156932</th>\n",
       "      <td>Babysight Digital Handheld Color Video Monitor</td>\n",
       "      <td>I purchased this and returned it immediately b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171309</th>\n",
       "      <td>Ecosusi Diaper Backpack, (pink)</td>\n",
       "      <td>I love this diaper bag. Everywhere I go with m...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57598</th>\n",
       "      <td>North American Bear Pastel Pancake Bear Plush ...</td>\n",
       "      <td>This is Pancake Bear number 4 for our house.  ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>Summer Infant Snuzzler Velboa for Head and Bod...</td>\n",
       "      <td>The Snuzzler is perfect for my baby boy. It ma...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54715 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "165191  aden + anais Rayon From Bamboo Crib Sheet, Azu...   \n",
       "108775  Hand Held Scalp Head Massager - Set of Three (...   \n",
       "162820             Clevamama ClevaFoam Baby Pillow, Cream   \n",
       "148217  Kids Line Velour Changing Pad Cover, Pirate Party   \n",
       "46428                      Angel Dear Blankie, Green Frog   \n",
       "...                                                   ...   \n",
       "137608  Trend Lab CribWrap Fleece Rail Cover for Long ...   \n",
       "156932     Babysight Digital Handheld Color Video Monitor   \n",
       "171309                    Ecosusi Diaper Backpack, (pink)   \n",
       "57598   North American Bear Pastel Pancake Bear Plush ...   \n",
       "99994   Summer Infant Snuzzler Velboa for Head and Bod...   \n",
       "\n",
       "                                                   review  rating  sentiments  \n",
       "165191  An off-white or cream sheet that is so soft. I...       5           1  \n",
       "108775  I was skeptical about how well these will work...       5           1  \n",
       "162820  It soft and material appears to be excellent. ...       5           1  \n",
       "148217  This is a very nice cover. I have two because ...       5           1  \n",
       "46428   I love these Lovies. They are cute, soft and d...       5           1  \n",
       "...                                                   ...     ...         ...  \n",
       "137608  Fit my Jenny Lind crib perfectly. Water proof ...       5           1  \n",
       "156932  I purchased this and returned it immediately b...       1           0  \n",
       "171309  I love this diaper bag. Everywhere I go with m...       5           1  \n",
       "57598   This is Pancake Bear number 4 for our house.  ...       5           1  \n",
       "99994   The Snuzzler is perfect for my baby boy. It ma...       5           1  \n",
       "\n",
       "[54715 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(34)\n",
    "df1 = df.sample(frac = 0.3)\n",
    "#Adding the sentiments column\n",
    "df1['sentiments'] = df1.rating.apply(lambda x: 0 if x in [1, 2,3] else 1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdd9fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1['review']\n",
    "y = df1['sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a49b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Logistic Regression with CountVectorizer\n",
      "0.8680093574091673\n",
      "4310 2217 1394 19437\n",
      "0.9331 0.6603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramni\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.5, random_state=24)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "#Vectorizing the text data\n",
    "ctmTr = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Training the model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(ctmTr, y_train)\n",
    "#Accuracy score\n",
    "lr_score = lr.score(X_test_dtm, y_test)\n",
    "print(\"Results for Logistic Regression with CountVectorizer\")\n",
    "print(lr_score)\n",
    "#Predicting the labels for test data\n",
    "y_pred_lr = lr.predict(X_test_dtm)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "#True positive and true negative rates\n",
    "tpr_lr = round(tp/(tp + fn), 4)\n",
    "tnr_lr = round(tn/(tn+fp), 4)\n",
    "print(tpr_lr, tnr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7171c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Support Vector Machine with CountVectorizer\n",
      "0.8582133196871117\n",
      "3399 3127 752 20080\n",
      "0.9639 0.5208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.5, random_state=123)\n",
    "#Vectorizing the text data\n",
    "cv = CountVectorizer()\n",
    "ctmTr = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "from sklearn import svm\n",
    "#Training the model\n",
    "svcl = svm.SVC()\n",
    "svcl.fit(ctmTr, y_train)\n",
    "svcl_score = svcl.score(X_test_dtm, y_test)\n",
    "print(\"Results for Support Vector Machine with CountVectorizer\")\n",
    "print(svcl_score)\n",
    "y_pred_sv = svcl.predict(X_test_dtm)\n",
    "#Confusion matrix\n",
    "cm_sv = confusion_matrix(y_test, y_pred_sv)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_sv).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "tpr_sv = round(tp/(tp + fn), 4)\n",
    "tnr_sv = round(tn/(tn+fp), 4)\n",
    "print(tpr_sv, tnr_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa4ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for KNN Classifier with CountVectorizer\n",
      "0.7710358944367278\n",
      "938 5545 719 20156\n",
      "0.9656 0.1447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.5, random_state=143)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "ctmTr = cv.fit_transform(X_train)\n",
    "X_test_dtm = cv.transform(X_test)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(ctmTr, y_train)\n",
    "knn_score = knn.score(X_test_dtm, y_test)\n",
    "print(\"Results for KNN Classifier with CountVectorizer\")\n",
    "print(knn_score)\n",
    "y_pred_knn = knn.predict(X_test_dtm)\n",
    "#Confusion matrix\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "tpr_knn = round(tp/(tp + fn), 4)\n",
    "tnr_knn = round(tn/(tn+fp), 4)\n",
    "print(tpr_knn, tnr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef7d255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Logistic Regression with tfidf\n",
      "0.8771108999195848\n",
      "3977 2531 831 20019\n",
      "0.9601 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramni\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.5, random_state=45)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vec, y_train)\n",
    "lr_score = lr.score(X_test_vec, y_test)\n",
    "print(\"Results for Logistic Regression with tfidf\")\n",
    "print(lr_score)\n",
    "y_pred_lr = lr.predict(X_test_vec)\n",
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_knn = confusion_matrix(y_test, y_pred_lr)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "tpr_knn = round(tp/(tp + fn), 4)\n",
    "tnr_knn = round(tn/(tn+fp), 4)\n",
    "print(tpr_knn, tnr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a23b85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Support Vector Machine with tfidf\n",
      "0.8770743475400249\n",
      "4038 2467 896 19957\n",
      "0.957 0.6208\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.5, random_state=55)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "from sklearn import svm\n",
    "#params = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100]}\n",
    "svcl = svm.SVC(kernel = 'rbf')\n",
    "#clf_sv = GridSearchCV(svcl, params)\n",
    "svcl.fit(X_train_vec, y_train)\n",
    "svcl_score = svcl.score(X_test_vec, y_test)\n",
    "print(\"Results for Support Vector Machine with tfidf\")\n",
    "print(svcl_score)\n",
    "y_pred_sv = svcl.predict(X_test_vec)\n",
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_sv = confusion_matrix(y_test, y_pred_sv)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_sv).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "tpr_sv = round(tp/(tp + fn), 4)\n",
    "tnr_sv = round(tn/(tn+fp), 4)\n",
    "print(tpr_sv, tnr_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476b03df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for KNN Classifier with tfidf\n",
      "0.781891951166021\n",
      "1668 4840 1127 19723\n",
      "0.9459 0.2563\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size = 0.5, random_state=65)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_vec, y_train)\n",
    "knn_score = knn.score(X_test_vec, y_test)\n",
    "print(\"Results for KNN Classifier with tfidf\")\n",
    "print(knn_score)\n",
    "y_pred_knn = knn.predict(X_test_vec)\n",
    "#Confusion matrix\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "tpr_knn = round(tp/(tp + fn), 4)\n",
    "tnr_knn = round(tn/(tn+fp), 4)\n",
    "print(tpr_knn, tnr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37992f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
